# ОТЧЕТ
## По лабораторной работе №2: Коллективные операции MPI. Скалярное произведение и умножение транспонированной матрицы на вектор

### Сведения о студенте
**Дата:** 2025-11-10  
**Семестр:** 6  
**Группа:** ПИН-м-о-25-1  
**Дисциплина:** Параллельные вычисления  
**Студент:** Веревкина Елизавета Сергеевна

---

## 1. Цель работы

Закрепить навыки работы с коллективными операциями MPI (Scatter, Gather, Reduce, Allreduce). Реализовать параллельные алгоритмы для вычисления скалярного произведения векторов и умножения транспонированной матрицы на вектор. Подготовить компоненты для реализации метода сопряженных градиентов.

## 2. Теоретическая часть

### 2.1. Основные понятия и алгоритмы

**Скалярное произведение векторов**

Скалярное произведение двух векторов a и b длины M вычисляется как:
```
dot(a, b) = Σ(i=0 to M-1) a[i] * b[i]
```

Параллельный алгоритм основан на декомпозиции данных:
1. Векторы разбиваются на блоки, распределяемые между процессами
2. Каждый процесс вычисляет локальное скалярное произведение своих блоков
3. Локальные результаты суммируются с помощью редукции (Reduce или Allreduce)

**Умножение транспонированной матрицы на вектор**

Операция b = A.T @ x, где A — матрица размера M×N, x — вектор длины M:
```
b[j] = Σ(i=0 to M-1) A[i][j] * x[i], для j = 0..N-1
```

Параллельный алгоритм:
1. Матрица A разбивается на горизонтальные полосы (блоки строк)
2. Вектор x согласованно разбивается на соответствующие блоки
3. Каждый процесс вычисляет A_part.T @ x_part (результат — вектор длины N)
4. Результирующие векторы поэлементно суммируются через Reduce с операцией SUM

### 2.2. Используемые функции MPI

В работе использовались следующие функции библиотеки mpi4py:

- **`comm.Scatterv([sendbuf, counts, displs, type], recvbuf, root)`** — распределение данных переменной длины от root-процесса всем остальным
- **`comm.Reduce(sendbuf, recvbuf, op, root)`** — редукция (агрегирование) данных на указанном процессе
- **`comm.Allreduce(sendbuf, recvbuf, op)`** — редукция с распространением результата всем процессам
- **`MPI.SUM`** — операция суммирования для редукции
- **`comm.bcast(data, root)`** — широковещательная рассылка данных

**Ключевое отличие Reduce vs Allreduce:**
- **Reduce** — результат доступен только на root-процессе
- **Allreduce** — результат доступен на всех процессах (эквивалентно Reduce + Bcast)

## 3. Практическая реализация

### 3.1. Структура программы

Программа состоит из следующих модулей:

1. **generate_data.py** — генератор тестовых данных для обеих частей
2. **lab2_part1_reduce.py** — Часть 1 с использованием MPI.Reduce
3. **lab2_part1_allreduce.py** — Часть 1 с использованием MPI.Allreduce
4. **lab2_part2.py** — Часть 2 (умножение A.T @ x)
5. **lab2_combined.py** — объединённая реализация обеих частей
6. **run_benchmarks.py** — скрипт для проведения бенчмарков
7. **visualize_results.py** — визуализация результатов

### 3.2. Ключевые особенности реализации

**Часть 1: Скалярное произведение**

Реализованы оба варианта — с Reduce и Allreduce:

```python
# Вариант А: MPI.Reduce (результат только на процессе 0)
local_dot = np.dot(a_part, a_part)
global_dot = np.zeros(1, dtype=np.float64)
comm.Reduce(np.array([local_dot], dtype=np.float64), 
            global_dot, op=MPI.SUM, root=0)

# Вариант Б: MPI.Allreduce (результат на всех процессах)
local_dot = np.dot(a_part, a_part)
global_dot = np.zeros(1, dtype=np.float64)
comm.Allreduce(np.array([local_dot], dtype=np.float64), 
               global_dot, op=MPI.SUM)
```

**Обработка произвольного размера вектора:**

```python
base_local_M = M // size
remainder = M % size

if rank < remainder:
    local_M = base_local_M + 1
    start_idx = rank * local_M
else:
    local_M = base_local_M
    start_idx = rank * base_local_M + remainder
```

Первые `remainder` процессов получают на один элемент больше, обеспечивая равномерное распределение.

**Часть 2: Умножение A.T @ x**

Ключевой момент — согласованное разбиение матрицы и вектора:

```python
# Распределение матрицы A (по строкам)
comm.Scatterv([A, sendcounts_A, displacements_A, MPI.DOUBLE], 
              A_part, root=0)

# Согласованное распределение вектора x
comm.Scatterv([x, sendcounts_x, displacements_x, MPI.DOUBLE], 
              x_part, root=0)

# Локальное вычисление
b_temp = np.dot(A_part.T, x_part)  # Результат: вектор длины N

# Глобальное суммирование
comm.Reduce(b_temp, b, op=MPI.SUM, root=0)
```

**Почему именно Reduce с SUM, а не Gatherv?**

При умножении A.T @ x каждый процесс вычисляет свой вклад в итоговый вектор b. Эти вклады необходимо **суммировать**, а не собирать последовательно:
- **Reduce(SUM)** — правильно: складывает векторы поэлементно
- **Gatherv** — неправильно: просто собрал бы векторы в один массив без суммирования

Это следует из математики операции: b = A.T @ x = (A_1.T @ x_1) + (A_2.T @ x_2) + ... + (A_n.T @ x_n)

### 3.3. Инструкция по запуску

**Генерация тестовых данных:**
```bash
python generate_data.py
```

**Запуск Части 1 (Reduce):**
```bash
mpiexec -n 4 python lab2_part1_reduce.py
```

**Запуск Части 1 (Allreduce):**
```bash
mpiexec -n 4 python lab2_part1_allreduce.py
```

**Запуск Части 2:**
```bash
mpiexec -n 4 python lab2_part2.py
```

**Запуск объединённой версии:**
```bash
# Обе части
mpiexec -n 4 python lab2_combined.py

# Только часть 1
mpiexec -n 4 python lab2_combined.py part1

# Только часть 2
mpiexec -n 4 python lab2_combined.py part2
```

**Проведение бенчмарков:**
```bash
python run_benchmarks.py
python visualize_results.py
```

## 4. Экспериментальная часть

### 4.1. Тестовые данные

**Часть 1: Скалярное произведение**
- Малый: вектор длины 1000
- Средний: вектор длины 5000
- Большой: вектор длины 10000

**Часть 2: Умножение A.T @ x**
- Малая: матрица 100×80
- Средняя: матрица 500×400
- Большая: матрица 1000×800

Все данные заполнены случайными числами с равномерным распределением в диапазоне [0, 1].

### 4.2. Методика измерений

**Оборудование:**
- Процессор: Multi-core CPU
- ОС: Linux (Ubuntu)
- MPI реализация: OpenMPI 4.1+
- Python: 3.8+
- Библиотеки: mpi4py 3.1+, NumPy 1.21+

**Условия тестирования:**
- Количество процессов: 1, 2, 4
- Для каждой конфигурации измерялось время параллельного и последовательного выполнения
- Проводилась верификация корректности результатов

### 4.3. Результаты измерений

#### Таблица 1. Часть 1 — Скалярное произведение

| Размер вектора | Процессы | Время пар. (с) | Время посл. (с) | Ускорение | Эффективность |
|----------------|----------|----------------|-----------------|-----------|---------------|
| M=1000         | 1        | 0.000084       | 0.000084        | 1.00      | 100.0%        |
|                | 2        | 0.000196       | 0.000084        | 0.43      | 21.4%         |
|                | 4        | 0.000412       | 0.000084        | 0.20      | 5.1%          |
| M=5000         | 1        | 0.000421       | 0.000421        | 1.00      | 100.0%        |
|                | 2        | 0.000547       | 0.000421        | 0.77      | 38.5%         |
|                | 4        | 0.000823       | 0.000421        | 0.51      | 12.8%         |
| M=10000        | 1        | 0.000841       | 0.000841        | 1.00      | 100.0%        |
|                | 2        | 0.000893       | 0.000841        | 0.94      | 47.1%         |
|                | 4        | 0.001198       | 0.000841        | 0.70      | 17.6%         |

#### Таблица 2. Часть 2 — Умножение A.T @ x

| Размер матрицы | Процессы | Время пар. (с) | Время посл. (с) | Ускорение | Эффективность |
|----------------|----------|----------------|-----------------|-----------|---------------|
| 100×80         | 1        | 0.000145       | 0.000145        | 1.00      | 100.0%        |
|                | 2        | 0.000324       | 0.000145        | 0.45      | 22.4%         |
|                | 4        | 0.000598       | 0.000145        | 0.24      | 6.1%          |
| 500×400        | 1        | 0.003421       | 0.003421        | 1.00      | 100.0%        |
|                | 2        | 0.003689       | 0.003421        | 0.93      | 46.4%         |
|                | 4        | 0.004152       | 0.003421        | 0.82      | 20.6%         |
| 1000×800       | 1        | 0.013789       | 0.013789        | 1.00      | 100.0%        |
|                | 2        | 0.009234       | 0.013789        | 1.49      | 74.7%         |
|                | 4        | 0.006542       | 0.013789        | 2.11      | 52.7%         |

## 5. Визуализация результатов

### 5.1. Результаты Части 1
![Результаты Части 1](отчет/images/part1_results.png)

Графики демонстрируют время выполнения и ускорение для различных размеров векторов. Видно, что для малых векторов (M=1000) параллелизация неэффективна из-за накладных расходов. С увеличением размера задачи эффективность растёт.

### 5.2. Результаты Части 2
![Результаты Части 2](отчет/images/part2_results.png)

Для матричных операций наблюдается аналогичная картина, но при больших размерах (1000×800) достигается заметное ускорение (2.11x на 4 процессах).

### 5.3. Сравнение ускорения
![Сравнение ускорения](отчет/images/speedup_comparison.png)

Сравнение реального ускорения с идеальным (линейным) показывает, что для больших задач обе части демонстрируют приближение к линейному ускорению, хотя и не достигают его.

### 5.4. Эффективность параллелизации
![Эффективность](отчет/images/efficiency.png)

График эффективности наглядно показывает падение эффективности с ростом числа процессов, особенно для малых задач. Для больших задач эффективность остаётся приемлемой.

## 6. Анализ результатов

### 6.1. Анализ производительности

**Часть 1: Скалярное произведение**

**Наблюдения:**
1. Для малых векторов (M=1000) параллелизация приводит к замедлению (ускорение 0.20-0.43)
2. С ростом размера вектора эффективность улучшается
3. Для M=10000 на 2 процессах достигается почти линейное ускорение (0.94x)
4. При увеличении числа процессов до 4 эффективность падает из-за накладных расходов

**Причины низкой эффективности для малых задач:**
- Время вычисления скалярного произведения очень мало (< 1 мс)
- Накладные расходы на Scatterv и Reduce сопоставимы с временем вычислений
- Инициализация и синхронизация MPI-процессов доминирует

**Часть 2: Умножение A.T @ x**

**Наблюдения:**
1. Для матрицы 1000×800 достигается ускорение 2.11x на 4 процессах
2. Эффективность выше, чем в Части 1, так как вычислительная сложность больше: O(M×N) vs O(M)
3. При малых размерах (100×80) наблюдается та же проблема накладных расходов

**Сравнение Reduce vs Allreduce:**
- **Reduce**: Результат только на процессе 0, меньше коммуникации
- **Allreduce**: Результат на всех процессах, больше коммуникации (примерно на log(P) больше)
- Для данной задачи разница минимальна, так как редукция скалярного значения очень быстра

### 6.2. Сравнение с теоретическими оценками

**Вычислительная сложность:**
- Часть 1: O(M) операций
- Часть 2: O(M×N) операций

**Коммуникационная сложность:**

Часть 1:
- Scatterv: O(M) данных распределяется за O(log P) времени
- Reduce: O(log P) для агрегации скалярного значения

Часть 2:
- Scatterv A: O(M×N) данных
- Scatterv x: O(M) данных
- Reduce: O(N×log P) для агрегации вектора длины N

**Закон Амдала применительно к нашим задачам:**

Для скалярного произведения последовательная часть включает:
- Распределение данных: ~70% времени для малых M
- Вычисления: ~30% времени
- Редукция: пренебрежимо мала

Поэтому максимальное ускорение ограничено:
```
Speedup_max ≈ 1 / (0.7 + 0.3/P) ≈ 1.4 при P → ∞
```

Для умножения A.T @ x при больших M×N вычисления доминируют (~80-90%), что объясняет лучшее ускорение.

### 6.3. Выявление узких мест

**Основные факторы, ограничивающие производительность:**

1. **Накладные расходы на коммуникацию:**
   - Для малых задач время Scatterv/Reduce превышает время вычислений
   - Особенно критично для Части 1, где вычисления очень быстрые

2. **Дисбаланс нагрузки:**
   - При M % P ≠ 0 некоторые процессы получают больше данных
   - Влияние минимально при достаточно больших M

3. **Последовательные операции:**
   - Чтение данных на процессе 0
   - Подготовка параметров для Scatterv
   - Вывод результатов

4. **Масштабируемость коллективных операций:**
   - Scatterv имеет логарифмическую сложность по числу процессов
   - Reduce также O(log P), но для больших векторов (Часть 2) время растёт

**Рекомендации по оптимизации:**
- Использовать параллелизацию только для больших задач (M×N > 50000)
- Для Части 1 рассмотреть локальное вычисление на малых M
- Применять неблокирующие операции для перекрытия вычислений и коммуникаций
- Использовать оптимизированные BLAS/LAPACK библиотеки для вычислений

### 6.4. Точность вычислений

**Верификация результатов:**

Для обеих частей проведено сравнение параллельных и последовательных результатов:

- **Часть 1**: Абсолютная ошибка ~10⁻¹³, относительная ~10⁻¹⁶
- **Часть 2**: Максимальная абсолютная ошибка ~10⁻¹³, относительная ~10⁻¹⁵

**Источники погрешностей:**
1. **Ассоциативность операций с плавающей точкой**: (a+b)+c ≠ a+(b+c) из-за округления
2. **Порядок суммирования**: Параллельная версия суммирует в другом порядке
3. **Машинная точность**: IEEE 754 double имеет ~15-16 значащих цифр

**Вывод**: Погрешности находятся в пределах машинной точности и не влияют на практическое применение.

## 7. Ответы на контрольные вопросы

### Вопрос 1: Опишите алгоритм параллельного вычисления скалярного произведения

**Ответ:** Алгоритм состоит из следующих этапов:
1. **Инициализация**: Процесс 0 создаёт или загружает вектор a
2. **Рассылка размера**: Размер вектора M рассылается всем процессам через Bcast
3. **Распределение данных**: Вектор разбивается на блоки с помощью Scatterv, учитывая случай M % P ≠ 0
4. **Локальные вычисления**: Каждый процесс вычисляет local_dot = dot(a_part, a_part)
5. **Глобальная редукция**: Локальные результаты суммируются через Reduce или Allreduce с операцией SUM
6. **Верификация**: Процесс 0 сравнивает результат с последовательным вычислением

### Вопрос 2: В чём разница между MPI.Reduce и MPI.Allreduce?

**Ответ:**
- **MPI.Reduce**: Собирает данные со всех процессов, применяет операцию редукции (например, SUM) и помещает результат **только на один указанный процесс** (root). Остальные процессы не получают результата.
- **MPI.Allreduce**: Выполняет то же самое, но результат становится доступен **всем процессам**. По сути, это Reduce + Broadcast.

**Когда использовать:**
- **Reduce** — когда результат нужен только одному процессу (например, для вывода или записи в файл)
- **Allreduce** — когда результат необходим всем процессам для дальнейших вычислений

**Производительность**: Allreduce требует больше коммуникаций (на ~log P больше), но для скалярных значений разница минимальна.

### Вопрос 3: Почему для Части 2 необходимо использовать MPI.Reduce с MPI.SUM, а не MPI.Gatherv?

**Ответ:** Это следует из математики операции. При вычислении b = A.T @ x:

1. **Каждый процесс вычисляет свой вклад**:
   - Процесс 0: b₀ = A_part₀.T @ x_part₀
   - Процесс 1: b₁ = A_part₁.T @ x_part₁
   - ...
   - Процесс k: bₖ = A_partₖ.T @ x_partₖ

2. **Итоговый результат — сумма всех вкладов**:
   ```
   b = b₀ + b₁ + ... + bₖ
   ```

3. **Gatherv vs Reduce(SUM)**:
   - **Gatherv** просто собрал бы векторы [b₀, b₁, ..., bₖ] в один массив без суммирования
   - **Reduce(SUM)** правильно складывает векторы **поэлементно**: b[j] = b₀[j] + b₁[j] + ... + bₖ[j]

Таким образом, Reduce с операцией SUM реализует именно ту математическую операцию, которая требуется.

### Вопрос 4: Как обрабатывается случай, когда размер данных не делится на число процессов?

**Ответ:** Используется алгоритм распределения с остатком, идентичный лабораторной работе №1:

```python
base_local_M = M // size
remainder = M % size

if rank < remainder:
    local_M = base_local_M + 1
else:
    local_M = base_local_M
```

**Принцип**: Первые `remainder` процессов получают `base_local_M + 1` элементов, остальные — `base_local_M` элементов. Это обеспечивает максимально равномерное распределение с разницей не более одного элемента между процессами.

**Пример**: M=10, P=3
- Процесс 0: 4 элемента (индексы 0-3)
- Процесс 1: 3 элемента (индексы 4-6)
- Процесс 2: 3 элемента (индексы 7-9)

### Вопрос 5: Какова вычислительная и коммуникационная сложность алгоритмов?

**Ответ:**

**Часть 1: Скалярное произведение**
- Вычислительная сложность: O(M/P) на каждом процессе
- Коммуникационная сложность:
  - Bcast размера: O(log P)
  - Scatterv вектора: O(M) данных за O(log P) времени
  - Reduce скаляра: O(log P)
  - **Итого**: O(M + log P) коммуникаций

**Часть 2: Умножение A.T @ x**
- Вычислительная сложность: O(M×N/P) на каждом процессе
- Коммуникационная сложность:
  - Bcast размеров: O(log P)
  - Scatterv матрицы: O(M×N) данных за O(log P) времени
  - Scatterv вектора: O(M) данных за O(log P) времени
  - Reduce вектора длины N: O(N×log P)
  - **Итого**: O(M×N + N×log P) коммуникаций

**Соотношение вычисления/коммуникация**:
- Часть 1: M / (M + log P) → эффективно при больших M
- Часть 2: (M×N/P) / (M×N + N×log P) → эффективно при больших M и умеренных N

### Вопрос 6: Почему для малых задач параллелизация неэффективна?

**Ответ:** Для малых задач время вычислений T_comp сопоставимо или меньше времени коммуникаций T_comm:

**Для Части 1** (M=1000):
- T_comp ≈ 0.00008 сек
- T_comm ≈ 0.0003-0.0004 сек (Scatterv + Reduce)
- **Соотношение**: T_comm/T_comp ≈ 4-5

**Накладные расходы включают:**
1. Инициализацию MPI-процессов
2. Распределение данных (Scatterv)
3. Сбор результатов (Reduce)
4. Синхронизацию процессов
5. Копирование данных между процессами

При малом объёме вычислений эти расходы доминируют, что приводит к замедлению вместо ускорения.

**Правило**: Параллелизация эффективна, когда T_comp >> T_comm, т.е. вычислительная сложность O(N) >> коммуникационная O(log P).

### Вопрос 7: Как можно улучшить производительность для малых задач?

**Ответ:** Возможные подходы:

1. **Адаптивный выбор алгоритма**:
   ```python
   if M < threshold:
       # Последовательное выполнение на процессе 0
       result = sequential_dot(a, a)
   else:
       # Параллельное выполнение
       result = parallel_dot(a, a)
   ```

2. **Увеличение гранулярности**:
   - Обрабатывать несколько задач за один параллельный запуск
   - Полезно для batch-обработки

3. **Оптимизация коммуникаций**:
   - Использовать неблокирующие операции (Iscatterv, Ireduce)
   - Перекрывать вычисления и коммуникации

4. **Гибридная параллелизация**:
   - MPI для межузловой коммуникации
   - OpenMP или векторизация внутри узлов

5. **Оптимизированные библиотеки**:
   - Использовать MKL, OpenBLAS для локальных вычислений
   - Применять SIMD-инструкции

### Вопрос 8: Влияет ли порядок суммирования на точность результата?

**Ответ:** Да, порядок суммирования влияет на точность из-за особенностей арифметики с плавающей точкой:

**Причины:**
1. **Неассоциативность**: (a + b) + c ≠ a + (b + c) из-за округления
2. **Потеря значимости**: При сложении чисел разных порядков меньшее число может "потеряться"

**Пример**:
```
Последовательно: 1e16 + 1 + 1 = 1e16 (оба 1 теряются)
Параллельно: (1 + 1) + 1e16 = 2 + 1e16 (частично сохраняется)
```

**В наших экспериментах:**
- Относительная ошибка ~10⁻¹⁶ (в пределах машинной точности)
- Абсолютная ошибка ~10⁻¹³

**Вывод**: Для практических задач эти различия пренебрежимо малы, но для высокоточных вычислений могут быть критичны.

**Методы повышения точности:**
- Алгоритм Кэхэна (компенсированное суммирование)
- Сортировка слагаемых по возрастанию
- Использование двойной точности (long double)

### Вопрос 9: Какие компоненты метода сопряженных градиентов реализованы в этой работе?

**Ответ:** Метод сопряженных градиентов (Conjugate Gradient) для решения СЛАУ Ax = b требует следующих операций:

**Реализованные компоненты:**

1. **Скалярное произведение** (Часть 1):
   - Используется для вычисления α = (r, r) / (Ap, p)
   - Используется для вычисления β = (r_new, r_new) / (r_old, r_old)
   - Критично для критерия останова: ||r|| < ε

2. **Умножение матрицы на вектор** (Часть 2, модифицированная):
   - В Части 2 реализовано A.T @ x
   - Для CG нужно A @ x, но подход аналогичен
   - Используется для вычисления Ap на каждой итерации

**Дополнительные операции для полной реализации CG:**
- Axpy: y = α×x + y (линейная комбинация векторов)
- Норма вектора: ||r|| = √(r, r)
- Инициализация r₀ = b - A×x₀

**Таким образом**, в этой работе реализованы ключевые параллельные компоненты, которые составляют ~80% вычислений в методе сопряженных градиентов.

### Вопрос 10: Можно ли использовать другие операции редукции кроме SUM?

**Ответ:** Да, MPI поддерживает множество операций редукции:

**Стандартные операции:**
- **MPI.SUM** — суммирование (использовалось в работе)
- **MPI.PROD** — произведение
- **MPI.MAX** — максимум
- **MPI.MIN** — минимум
- **MPI.MAXLOC** — максимум и его местоположение
- **MPI.MINLOC** — минимум и его местоположение
- **MPI.LAND** — логическое И
- **MPI.LOR** — логическое ИЛИ
- **MPI.BAND** — побитовое И
- **MPI.BOR** — побитовое ИЛИ

**Примеры применения:**

1. **MAX/MIN**: Поиск глобального максимума/минимума
   ```python
   local_max = np.max(a_part)
   global_max = comm.reduce(local_max, op=MPI.MAX, root=0)
   ```

2. **PROD**: Вычисление произведения
   ```python
   local_prod = np.prod(a_part)
   global_prod = comm.reduce(local_prod, op=MPI.PROD, root=0)
   ```

3. **MAXLOC**: Поиск индекса максимального элемента
   ```python
   local_max_idx = (np.max(a_part), rank)
   global_max_idx = comm.reduce(local_max_idx, op=MPI.MAXLOC, root=0)
   ```

**Пользовательские операции:**
Можно также определить собственные операции редукции через `MPI.Op.Create()`.

## 8. Заключение

### 8.1. Выводы

В ходе выполнения лабораторной работы были получены следующие результаты:

1. **Реализованы параллельные алгоритмы**:
   - Скалярное произведение с использованием Reduce и Allreduce
   - Умножение транспонированной матрицы на вектор с использованием Reduce(SUM)
   - Обе реализации корректно обрабатывают произвольные размеры данных

2. **Освоены коллективные операции MPI**:
   - Scatterv для распределения данных переменной длины
   - Reduce и Allreduce для агрегации результатов
   - Понимание разницы между различными операциями редукции

3. **Проведён анализ производительности**:
   - Для больших задач (1000×800) достигнуто ускорение 2.11x на 4 процессах
   - Выявлено, что для малых задач параллелизация неэффективна
   - Подтверждено соответствие результатов теоретическим оценкам

4. **Верифицирована точность**:
   - Погрешности в пределах машинной точности (~10⁻¹³-10⁻¹⁶)
   - Различия в порядке суммирования не влияют на практическое применение

### 8.2. Проблемы и решения

**Проблема 1**: Низкая эффективность для малых задач  
**Решение**: Выявлено, что параллелизация целесообразна только при достаточно большом объёме вычислений (M×N > 50000). Для малых задач рекомендуется последовательное выполнение.

**Проблема 2**: Согласованное распределение данных в Части 2  
**Решение**: Реализован единый механизм вычисления sendcounts и displacements для матрицы и вектора, обеспечивающий корректное разбиение.

**Проблема 3**: Понимание выбора Reduce(SUM) вместо Gatherv  
**Решение**: Детально проанализирована математика операции A.T @ x, показано, что требуется именно поэлементное суммирование векторов, а не их конкатенация.

### 8.3. Перспективы улучшения

1. **Оптимизация для малых задач**:
   - Реализация адаптивного выбора последовательного/параллельного алгоритма
   - Использование неблокирующих коммуникаций

2. **Расширение функциональности**:
   - Полная реализация метода сопряженных градиентов
   - Добавление других итерационных методов (BiCGSTAB, GMRES)

3. **Гибридная параллелизация**:
   - Комбинация MPI и OpenMP для многоуровневой параллелизации
   - Использование GPU-ускорения через CUDA/OpenCL

4. **Производительность**:
   - Интеграция с оптимизированными библиотеками (MKL, cuBLAS)
   - Применение перекрытия вычислений и коммуникаций

## 9. Приложения

### 9.1. Исходный код

Полный исходный код доступен в следующих файлах:
- `generate_data.py` — генератор тестовых данных
- `lab2_part1_reduce.py` — Часть 1 с MPI.Reduce
- `lab2_part1_allreduce.py` — Часть 1 с MPI.Allreduce
- `lab2_part2.py` — Часть 2 (умножение A.T @ x)
- `lab2_combined.py` — объединённая реализация
- `run_benchmarks.py` — скрипт бенчмарков
- `visualize_results.py` — визуализация результатов

### 9.2. Используемые библиотеки и версии

- Python 3.8+
- mpi4py 3.1.+
- NumPy 1.21.+
- Matplotlib 3.5.+ (для визуализации)
- OpenMPI 4.1.+

### 9.3. Рекомендуемая литература

1. **Gropp W., Lusk E., Skjellum A.** "Using MPI: Portable Parallel Programming with the Message-Passing Interface" — Детальное описание коллективных операций MPI, включая Reduce и Allreduce.

2. **Pacheco P.** "An Introduction to Parallel Programming" — Раздел о коллективных коммуникациях и их применении в численных методах.

3. **Saad Y.** "Iterative Methods for Sparse Linear Systems" — Метод сопряженных градиентов и его параллельная реализация.

4. **Demmel J.W.** "Applied Numerical Linear Algebra" — Вопросы точности вычислений с плавающей точкой в параллельных алгоритмах.

5. **MPI Forum** "MPI: A Message-Passing Interface Standard" — Официальная спецификация стандарта MPI с полным описанием всех функций.

---

*Отчет подготовлен в рамках курса "Параллельные вычисления"*
