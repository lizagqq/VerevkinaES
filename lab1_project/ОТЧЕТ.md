# ОТЧЕТ
## По лабораторной работе №1: Параллельное умножение матрицы на вектор с использованием MPI в Python

### Сведения о студенте
**Дата:** 2025-11-07  
**Семестр:** 6  
**Группа:** ПИН-м-о-25-1 
**Дисциплина:** Параллельные вычисления  
**Студент:** Веревкина Елизавета Сергеевна

---

## 1. Цель работы

Освоить базовые принципы программирования в модели передачи сообщений (Message Passing) с использованием библиотеки mpi4py для Python. Получить практические навыки распараллеливания вычислительной задачи (умножение матрицы на вектор) на системы с распределенной памятью.

## 2. Теоретическая часть

### 2.1. Основные понятия и алгоритмы

Умножение матрицы A размерности M×N на вектор x длины N является типичной вычислительно ёмкой операцией линейной алгебры. Результатом является вектор b длины M, где каждый элемент b[i] вычисляется как скалярное произведение i-й строки матрицы A на вектор x:

```
b[i] = Σ(j=0 to N-1) A[i][j] * x[j]
```

**Параллельный алгоритм** основан на декомпозиции данных по строкам матрицы:

1. **Декомпозиция данных**: Матрица A разбивается на горизонтальные полосы (блоки строк). Каждый процесс получает свой блок размером local_M × N.

2. **Распределение данных**: Главный процесс (rank 0) распределяет блоки матрицы между всеми процессами. Вектор x рассылается всем процессам целиком.

3. **Локальные вычисления**: Каждый процесс независимо вычисляет свою часть результирующего вектора: b_part = A_part × x.

4. **Сбор результатов**: Частичные результаты собираются на главном процессе в итоговый вектор b.

### 2.2. Используемые функции MPI

В работе использовались следующие функции библиотеки mpi4py:

- **`MPI.COMM_WORLD.Get_rank()`** — получение ранга (идентификатора) текущего процесса
- **`MPI.COMM_WORLD.Get_size()`** — получение общего количества процессов
- **`comm.bcast(data, root=0)`** — широковещательная рассылка данных от процесса root всем остальным
- **`comm.Send(data, dest, tag)`** — отправка данных процессу с рангом dest
- **`comm.Recv(buffer, source, tag)`** — приём данных от процесса с рангом source
- **`comm.Scatterv([sendbuf, counts, displs, type], recvbuf, root)`** — распределение данных переменной длины
- **`comm.Gatherv(sendbuf, [recvbuf, counts, displs, type], root)`** — сбор данных переменной длины

## 3. Практическая реализация

### 3.1. Структура программы

Программа состоит из нескольких модулей:

1. **generate_data.py** — генератор тестовых данных
2. **lab1_sequential.py** — последовательная версия для верификации
3. **lab1_parallel_sendrecv.py** — базовая параллельная версия с Send/Recv
4. **lab1_parallel_collective.py** — оптимизированная версия с коллективными операциями
5. **lab1_parallel_arbitrary.py** — версия с поддержкой произвольного размера матрицы
6. **simple_benchmark.py** — скрипт для проведения бенчмарков
7. **visualize_results.py** — визуализация результатов

### 3.2. Ключевые особенности реализации

**Этап 1: Последовательная версия**

Реализована базовая версия без использования MPI для верификации корректности параллельных версий:

```python
def sequential_matrix_vector_multiply():
    # Чтение размеров
    with open('in.dat', 'r') as f:
        M, N = map(int, f.readline().split())
    
    # Чтение матрицы A и вектора x
    A = np.loadtxt('AData.dat').reshape(M, N)
    x = np.loadtxt('xData.dat')
    
    # Умножение матрицы на вектор
    b = np.dot(A, x)
    
    # Сохранение результата
    np.savetxt('Results_sequential.dat', b, fmt='%.6f')
    return b
```

**Этап 2: Базовая параллельная версия**

Использует операции точка-точка (Send/Recv) для распределения и сбора данных. Работает корректно только когда M делится на количество процессов без остатка.

**Этап 3: Оптимизация с коллективными операциями**

Замена Send/Recv на Scatterv/Gatherv повышает эффективность за счёт оптимизированных коллективных операций MPI.

**Этап 4: Обработка произвольного размера**

Ключевое улучшение — корректная работа при любом соотношении M и количества процессов:

```python
# Вычисление распределения строк между процессами
base_local_M = M // size
remainder = M % size

if rank < remainder:
    local_M = base_local_M + 1
    start_row = rank * local_M
else:
    local_M = base_local_M
    start_row = rank * base_local_M + remainder
```

Первые `remainder` процессов получают на одну строку больше, обеспечивая равномерное распределение нагрузки.

### 3.3. Инструкция по запуску

**Генерация тестовых данных:**
```bash
python generate_data.py
```

**Запуск последовательной версии:**
```bash
python lab1_sequential.py
```

**Запуск параллельных версий:**
```bash
# Базовая версия с Send/Recv (M должно делиться на n)
mpiexec -n 4 python lab1_parallel_sendrecv.py

# Версия с коллективными операциями
mpiexec -n 4 python lab1_parallel_collective.py

# Версия с произвольным размером
mpiexec -n 4 python lab1_parallel_arbitrary.py
```

**Проведение бенчмарков:**
```bash
python simple_benchmark.py
python visualize_results.py
```

## 4. Экспериментальная часть

### 4.1. Тестовые данные

Проведено тестирование на матрицах трёх размеров:
- Малая: 100 × 100 (10 000 элементов)
- Средняя: 500 × 500 (250 000 элементов)
- Большая: 1000 × 1000 (1 000 000 элементов)

Матрицы заполнены случайными числами с равномерным распределением в диапазоне [0, 1].

### 4.2. Методика измерений

**Оборудование:**
- Процессор: Multi-core CPU
- ОС: Linux (Ubuntu)
- MPI реализация: OpenMPI 4.1+
- Python: 3.8+
- Библиотеки: mpi4py 3.1+, NumPy 1.21+

**Условия тестирования:**
- Количество процессов: 1, 2, 4, 8
- Для каждой конфигурации проводилось измерение времени выполнения
- Использовалась версия с произвольным размером матрицы (lab1_parallel_arbitrary.py)

### 4.3. Результаты измерений

#### Таблица 1. Время выполнения (секунды)

| Количество процессов | 100×100   | 500×500   | 1000×1000 |
|---------------------|-----------|-----------|-----------|
| 1                   | 0.000215  | 0.005234  | 0.021456  |
| 2                   | 0.000398  | 0.003876  | 0.012347  |
| 4                   | 0.000621  | 0.002914  | 0.007128  |
| 8                   | 0.001142  | 0.002487  | 0.004756  |

#### Таблица 2. Ускорение (Speedup)

| Количество процессов | 100×100   | 500×500   | 1000×1000 |
|---------------------|-----------|-----------|-----------|
| 1                   | 1.00      | 1.00      | 1.00      |
| 2                   | 0.54      | 1.35      | 1.74      |
| 4                   | 0.35      | 1.80      | 3.01      |
| 8                   | 0.19      | 2.10      | 4.51      |

#### Таблица 3. Эффективность (%)

| Количество процессов | 100×100   | 500×500   | 1000×1000 |
|---------------------|-----------|-----------|-----------|
| 1                   | 100.0     | 100.0     | 100.0     |
| 2                   | 27.0      | 68.0      | 87.0      |
| 4                   | 9.0       | 45.0      | 75.0      |
| 8                   | 2.0       | 26.0      | 56.0      |

## 5. Визуализация результатов

### 5.1. График времени выполнения
![График времени выполнения](images/execution_time.png)

График демонстрирует зависимость времени выполнения от количества процессов для трёх размеров матриц. Видно, что для малых матриц время увеличивается с ростом числа процессов из-за накладных расходов на коммуникацию.

### 5.2. График ускорения
![График ускорения](images/speedup.png)

Сравнение реального ускорения с идеальным (линейным). Для больших матриц наблюдается приближение к линейному ускорению, в то время как для малых матриц ускорение практически отсутствует.

### 5.3. График эффективности
![График эффективности](images/efficiency.png)

Эффективность параллелизации падает с увеличением количества процессов, особенно заметно для малых задач. Для матрицы 1000×1000 эффективность остаётся приемлемой даже при 8 процессах (56%).

## 6. Анализ результатов

### 6.1. Анализ производительности

**Наблюдения:**

1. **Зависимость от размера задачи**: 
   - Для матрицы 100×100 параллелизация неэффективна — время выполнения увеличивается с ростом числа процессов
   - Для матрицы 500×500 наблюдается умеренное ускорение (до 2.1x на 8 процессах)
   - Для матрицы 1000×1000 достигается значительное ускорение (4.51x на 8 процессах)

2. **Накладные расходы на коммуникацию**:
   - Время инициализации MPI, распределения и сбора данных составляет значительную часть для малых задач
   - С увеличением размера задачи доля вычислений растёт, что повышает эффективность параллелизации

3. **Масштабируемость**:
   - Программа демонстрирует хорошую масштабируемость для больших задач
   - Для малых задач наблюдается отрицательная масштабируемость

### 6.2. Сравнение с теоретическими оценками

Согласно **закону Амдала**, максимальное ускорение ограничено последовательной частью программы:

```
Speedup = 1 / (s + p/n)
```

где s — доля последовательных вычислений, p — доля параллельных вычислений, n — количество процессов.

В нашей задаче последовательная часть включает:
- Чтение данных (выполняется только на процессе 0)
- Коммуникационные операции (Bcast, Scatterv, Gatherv)

Для больших матриц доля вычислений p → 1, что объясняет приближение к линейному ускорению.

**Коммуникационная сложность:**
- Broadcast вектора x: O(N × log(n))
- Scatterv матрицы: O(M × N / n)
- Gatherv результата: O(M / n)

Общая коммуникационная сложность растёт логарифмически с числом процессов, что подтверждается экспериментальными данными.

### 6.3. Выявление узких мест

**Основные факторы, ограничивающие производительность:**

1. **Коммуникационные затраты**:
   - Для малых задач время коммуникации превышает время вычислений
   - Особенно критично для операций Scatterv/Gatherv при работе с маленькими блоками данных

2. **Несбалансированность нагрузки**:
   - При M % size ≠ 0 некоторые процессы получают на одну строку больше
   - Влияние незначительно при достаточно больших M

3. **Накладные расходы MPI**:
   - Инициализация MPI
   - Синхронизация процессов
   - Копирование данных между процессами

4. **Последовательные операции**:
   - Чтение файлов на процессе 0
   - Запись результатов на процессе 0

**Рекомендации по оптимизации:**
- Использовать параллелизацию только для больших задач (M × N > 100 000)
- Рассмотреть параллельный ввод-вывод (MPI-IO)
- Применять неблокирующие операции для перекрытия вычислений и коммуникаций

## 7. Ответы на контрольные вопросы

### Вопрос 1: Какие основные этапы включает параллельный алгоритм умножения матрицы на вектор?

**Ответ:** Параллельный алгоритм включает четыре основных этапа:
1. **Декомпозиция данных** — разбиение матрицы на блоки строк
2. **Распределение данных** — рассылка блоков матрицы и вектора процессам
3. **Локальные вычисления** — независимое вычисление частичных результатов каждым процессом
4. **Сбор результатов** — объединение частичных результатов в итоговый вектор

### Вопрос 2: В чём разница между операциями Send/Recv и коллективными операциями Scatterv/Gatherv?

**Ответ:** 
- **Send/Recv** — операции точка-точка, требуют явного указания отправителя и получателя для каждой пары процессов. При распределении данных между n процессами требуется n-1 пар операций Send/Recv, выполняемых последовательно или требующих сложной логики.
- **Scatterv/Gatherv** — коллективные операции, автоматически оптимизируют распределение/сбор данных переменной длины между всеми процессами. Используют эффективные алгоритмы (например, дерево бинарного распределения), обеспечивая лучшую производительность и простоту кода.

### Вопрос 3: Почему для малых матриц параллелизация неэффективна?

**Ответ:** Для малых матриц время вычислений сопоставимо или меньше времени коммуникации. Накладные расходы включают:
- Инициализацию MPI-процессов
- Распределение данных (Broadcast, Scatterv)
- Сбор результатов (Gatherv)
- Синхронизацию процессов

При малом объёме вычислений эти расходы превышают выигрыш от параллелизации, что приводит к замедлению вместо ускорения.

### Вопрос 4: Как обрабатывается случай, когда количество строк матрицы не делится на число процессов?

**Ответ:** Используется алгоритм распределения с остатком:
```python
base_local_M = M // size
remainder = M % size
```
Первые `remainder` процессов получают `base_local_M + 1` строк, остальные — `base_local_M` строк. Это обеспечивает максимально равномерное распределение нагрузки с разницей не более одной строки между процессами.

### Вопрос 5: Что такое ускорение (Speedup) и эффективность (Efficiency)?

**Ответ:**
- **Ускорение (Speedup)** — отношение времени выполнения последовательной версии к времени выполнения параллельной версии: S = T₁ / Tₙ, где T₁ — время на одном процессе, Tₙ — время на n процессах.
- **Эффективность (Efficiency)** — отношение ускорения к количеству процессов: E = S / n. Показывает, насколько эффективно используются вычислительные ресурсы (идеально E = 1 или 100%).

### Вопрос 6: Почему используется функция Bcast для распределения вектора x?

**Ответ:** Вектор x должен быть доступен полностью на каждом процессе для выполнения локального умножения A_part × x. Функция Bcast (broadcast) эффективно рассылает одни и те же данные от корневого процесса всем остальным процессам, используя оптимизированный алгоритм распространения (обычно в виде дерева), что быстрее, чем последовательные операции Send.

### Вопрос 7: Какова коммуникационная сложность алгоритма?

**Ответ:** Коммуникационная сложность складывается из:
- **Bcast вектора x**: O(N × log(size)) — логарифмическая зависимость от числа процессов
- **Scatterv блоков матрицы**: O(M × N) — общий объём данных матрицы
- **Gatherv результатов**: O(M) — размер результирующего вектора

Суммарная коммуникационная сложность: O(M × N + N × log(size) + M). Для больших M и N доминирует слагаемое M × N.

### Вопрос 8: Можно ли улучшить производительность, изменив способ декомпозиции?

**Ответ:** Да, возможны альтернативные подходы:
1. **Блочная декомпозиция** — разбиение матрицы на блоки (не только по строкам), может улучшить локальность данных
2. **Циклическое распределение** — распределение строк циклически (процесс i получает строки i, i+size, i+2×size, ...), обеспечивает лучшую балансировку при неравномерной вычислительной нагрузке
3. **2D декомпозиция** — разбиение и матрицы, и вектора, используется в библиотеках ScaLAPACK для очень больших задач

Для данной задачи блочная декомпозиция по строкам оптимальна по соотношению простоты реализации и эффективности.

### Вопрос 9: Какие факторы влияют на масштабируемость программы?

**Ответ:** Основные факторы:
1. **Размер задачи** — чем больше M×N, тем лучше масштабируемость
2. **Соотношение вычислений/коммуникаций** — должно быть в пользу вычислений
3. **Качество балансировки нагрузки** — равномерное распределение работы между процессами
4. **Архитектура системы** — пропускная способность сети, латентность коммуникаций
5. **Алгоритм коммуникации** — использование эффективных коллективных операций
6. **Накладные расходы MPI** — время инициализации и синхронизации

### Вопрос 10: Как можно верифицировать корректность параллельной программы?

**Ответ:** Методы верификации:
1. **Сравнение с последовательной версией** — наиболее надёжный метод, реализованный в данной работе
2. **Проверка с известным решением** — использование тестовых данных с заранее известным результатом
3. **Тестирование на различных конфигурациях** — проверка работы с разным количеством процессов и размерами матриц
4. **Проверка инвариантов** — например, норма результирующего вектора должна соответствовать теоретическим оценкам
5. **Использование отладочных инструментов** — Intel Inspector, Valgrind для проверки корректности работы с памятью

## 8. Заключение

### 8.1. Выводы

В ходе выполнения лабораторной работы были получены следующие результаты:

1. **Реализованы четыре версии программы**:
   - Последовательная версия для верификации
   - Базовая параллельная версия с операциями Send/Recv
   - Оптимизированная версия с коллективными операциями
   - Универсальная версия с поддержкой произвольного размера матрицы

2. **Освоены ключевые концепции MPI**:
   - Операции точка-точка (Send/Recv)
   - Коллективные операции (Bcast, Scatterv, Gatherv)
   - Декомпозиция данных и балансировка нагрузки

3. **Проведён анализ производительности**:
   - Для матрицы 1000×1000 достигнуто ускорение 4.51x на 8 процессах
   - Выявлена зависимость эффективности от размера задачи
   - Подтверждено соответствие результатов закону Амдала

4. **Установлены практические ограничения**:
   - Параллелизация эффективна только для достаточно больших задач
   - Коммуникационные расходы являются основным ограничивающим фактором

### 8.2. Проблемы и решения

**Проблема 1:** Деление матрицы при M % size ≠ 0  
**Решение:** Реализован алгоритм распределения с остатком, где первые процессы получают на одну строку больше.

**Проблема 2:** Сложность отладки MPI-программ  
**Решение:** Последовательная разработка с постепенным усложнением, верификация каждой версии относительно последовательной.

**Проблема 3:** Низкая производительность для малых матриц  
**Решение:** Выявлено, что параллелизация целесообразна только при M × N > 100 000.

### 8.3. Перспективы улучшения

1. **Оптимизация коммуникаций**:
   - Использование неблокирующих операций (Isend/Irecv)
   - Перекрытие вычислений и коммуникаций

2. **Параллельный ввод-вывод**:
   - Использование MPI-IO для распределённого чтения больших файлов
   - Параллельная запись результатов

3. **Гибридная параллелизация**:
   - Комбинация MPI (межузловая) и OpenMP (внутриузловая)
   - Использование SIMD-инструкций для векторизации

4. **Адаптивный выбор стратегии**:
   - Автоматическое определение оптимального количества процессов в зависимости от размера задачи
   - Динамическая балансировка нагрузки

## 9. Приложения

### 9.1. Исходный код

Полный исходный код доступен в следующих файлах:
- `generate_data.py` — генератор тестовых данных
- `lab1_sequential.py` — последовательная версия
- `lab1_parallel_sendrecv.py` — параллельная версия с Send/Recv
- `lab1_parallel_collective.py` — версия с коллективными операциями
- `lab1_parallel_arbitrary.py` — версия с произвольным размером
- `simple_benchmark.py` — скрипт бенчмарков
- `visualize_results.py` — визуализация результатов

### 9.2. Используемые библиотеки и версии

- Python 3.8+
- mpi4py 3.1.+
- NumPy 1.21.+
- Matplotlib 3.5.+ (для визуализации)
- OpenMPI 4.1.+

### 9.3. Рекомендуемая литература

1. **Gropp W., Lusk E., Skjellum A.** "Using MPI: Portable Parallel Programming with the Message-Passing Interface" — Фундаментальное руководство по MPI, подробно описывает все аспекты программирования с передачей сообщений.

2. **Pacheco P.** "An Introduction to Parallel Programming" — Отличное введение в параллельное программирование, включая разделы по MPI и практические примеры.

3. **Dalcin L. et al.** "mpi4py: MPI for Python" — Официальная документация библиотеки mpi4py, содержит примеры использования всех функций.

4. **Богачёв К.Ю.** "Основы параллельного программирования" — Российский учебник, содержащий теоретические основы и практические примеры на MPI.

5. **Foster I.** "Designing and Building Parallel Programs" — Методология проектирования параллельных программ, включая паттерны декомпозиции данных.

---

*Отчет подготовлен в рамках курса "Параллельные вычисления"*
