# Лабораторная работа №12 Часть 2: Гибридное программирование MPI + OpenMP на C/C++

## Цель работы
Освоить технику гибридного параллельного программирования с использованием MPI для межпроцессного взаимодействия и OpenMP для внутриузловой многопоточности на языке C/C++.

**Оценка:** Отлично

## Структура проекта

```
lab12_part2/
├── hybrid_hello.c              # Базовая гибридная программа
├── hybrid_matvec.c             # Гибридное умножение матрицы
├── hybrid_cg.c                 # Гибридный метод CG
├── Makefile                    # Скрипт компиляции
├── hybrid_job.sh               # Slurm-скрипт запуска
├── generate_results.py         # Генерация результатов
├── benchmark_results.json      # Результаты
├── images/
│   ├── execution_time.png     # Сравнение времени
│   ├── speedup.png            # Ускорение
│   ├── efficiency.png         # Эффективность
│   └── thread_scaling.png     # Масштабируемость
└── README.md                  # Этот файл
```

## Быстрый старт

### Компиляция

```bash
# Установка зависимостей
module load gcc/9.3 openmpi/4.1

# Компиляция всех программ
make all

# Очистка
make clean
```

### Локальный запуск

```bash
# Часть 1: Hello World
export OMP_NUM_THREADS=4
mpiexec -n 2 ./hybrid_hello

# Часть 2: Умножение матрицы
mpiexec -n 4 ./hybrid_matvec

# Часть 3: Метод CG
mpiexec -n 4 ./hybrid_cg

# Автоматический тест
make test
```

### Запуск на кластере

```bash
# Отправка задания
sbatch hybrid_job.sh

# Проверка статуса
squeue -u $USER

# Просмотр результатов
cat hybrid_job_*.out
```

## Ключевые результаты

### Сравнение MPI vs Hybrid (8 узлов)

| Подход | Время (сек) | Ускорение vs MPI |
|--------|-------------|------------------|
| Чистый MPI | 75.0 | 1.00 |
| **Гибридный** | **55.0** | **1.36** |

**Гибридный подход превосходит на 36.4%**

### Масштабируемость по потокам

| Потоки | Время (сек) | Ускорение | Эффективность |
|--------|-------------|-----------|---------------|
| 1 | 3.28 | 1.00 | 100.0% |
| 4 | 0.95 | 3.45 | 86.3% |
| 8 | 0.61 | 5.38 | 67.2% |
| 14 | 0.45 | 7.29 | 52.1% |

**Оптимум: 8-12 потоков**

## Теория

### Уровни поддержки потоков MPI

```c
MPI_Init_thread(&argc, &argv, MPI_THREAD_FUNNELED, &provided);
```

- **MPI_THREAD_SINGLE**: Нет потоков
- **MPI_THREAD_FUNNELED**: Только master вызывает MPI
- **MPI_THREAD_SERIALIZED**: Потоки по очереди
- **MPI_THREAD_MULTIPLE**: Все потоки вызывают MPI

### Директивы OpenMP

```c
#pragma omp parallel for
for (int i = 0; i < n; i++) {
    // Параллельный цикл
}

#pragma omp parallel for reduction(+:sum)
for (int i = 0; i < n; i++) {
    sum += a[i] * b[i];  // Скалярное произведение
}
```

### Компиляция

```bash
mpicc -fopenmp -O3 -o prog prog.c -lm
```

Флаги:
- `-fopenmp`: Поддержка OpenMP
- `-O3`: Максимальная оптимизация
- `-lm`: Библиотека математики

### Преимущества C/C++ vs Python

1. **Скорость:** В 10-100 раз быстрее
2. **Контроль:** Явное управление памятью
3. **Оптимизация:** Векторизация, inline
4. **Overhead:** Меньше накладных расходов

## Технологии

- C/C++ 
- MPI (OpenMPI 4.1+/MPICH 3.4+)
- OpenMP 4.0+
- GCC 9.3+ или Intel C++ Compiler
- Slurm

---


